Reactive apps with Reactor

== Part 2 
== Building a reactive Application : Démo : Brian Clozel
Spring boot reactive starter
Spring boot
Sptin 5.0
Reactor 3.0

Pour le web
Pas de dépendance avec l'api Servlet. On peut utiliser netty, tomcat etc...
Le plus important : Gérer le backpressure sur le réseau

 * Appeler du REST
 * dealing with datastores
 * Error handling
 * rendering HTML
 etc..

You can do sync in a Controller , no problem

in a class @RequestController



@GetMapping("/devoxx")
public String tets() {
return "hello";
}



@GetMapping("/devoxx_async")
public Mono<String> tets() {
       return Mono.just("hello");
}


// under the hood
@GetMapping("/devoxx_rawlevel")
public Mono<Void> exchange(ServerWebExchange exchange) {
       ServerHttpResponse  response = exchnge.getresponse();
       response.setStatusCode(HttpStatus.OK);
       // etc..
       // write with a publisher
       // DataBuffer : low level efficient way too deal with buffers, with no copy etc..
       private final DataBufferFactory factory = new DefaultDataBufferFactory();
       factory.allocateBuffer(à.write("Exchange : ".getBytes(StandardCharsets.UTF8);
       //response.writeWith(Publisher<DataBuffer> publisher) : signature
       response.writeWith(Mono.just(buffer))
}



// Exemple to wake the Mono wait
public Mono<String> waiting(){
       return Mono.never();
}
// Exemple to wake the Mono sends an error
public Mono<String> error_ing(){
       return Mono.error();
}




Monday 07/11/2016 13PM
= UNVERVERSITY KUBERNETES =
 * Mandy Waite @tekgrrl
 * Ray Stang @saturnism
 * James Strachan @jstrachan


Plan :
== Art of scheduling ==

== Kubernetes in Depth

== Developing in ka8s with continuous Delivery




== Art of scheduling ==


=== History
Google search /day
december 98 : 50.000 /day
April 99 : 500.000/day

Issue at Google : scalability
From chroot -> bsd jails  -> solaris zones

cgroups namespaces capacibilities chroots

encapsulate apps on a machine
Borg :work multiple tasks on a machine
Working production workload and batch workload on the same machine will increase machine usage. You do not loose 30% of the machine

limit : Ask amount of ressource requested
reservation : estimate future usage
reservation < limit
limit - reservation : CPU that can be reused

Bin-packing algorithms : to use more CPU and more memory. Fill gaps of memory usage and cpu


=== Important, wrap up :
* optimize resourses
* priorization
* sharing resources
* overcommit
* smarter scheduling

Application centric and not machine centric

* Over 2B containers launched/day


Containers and Kubernetes is needed by Google.


=== Advantages of containers
lightweight
hermeticcaly sealed
islated
easy to deploy
introspectable
runnable
Linux processes !
 * improces developper experience
  * simplifies operations
  Containers

"If you begin to run a container you run lots of containers"

==== Role of API
API to connect to run on google platform, ibm platform etc...


=== Kubernetes
Lots of containers -> K -> Scheduled and organised containers

* Schecheduling
* Lifecylcle and health
* Scaling
* Naming and discovery
* loadbalancing
* storage volumes
* logging and monitoring : most important in Borg, priority 0 , most imrtant, can't be preempted
*
 debuggung
 * identity, authorisation

K is 3 of three logs of Google Native
K is written more outside of Google

K is stable, from the experience of Borg
Tested thorouhgly
V1. Breaking changes kept to v2
healthy ecosystem


==== primitives
* ingess
* ...



== Architecture
Developpers => Control pane => (through cluster) => multiple Kubelet 


=== How to start
With a cluster. Laptop or real
Hosted of self managed


==== Set up a cluster
easier now
* linux-on-prem : run kubeadm command : creat nodes, configure master etc???
* AWS : KOPS
* Google : Google cloud : see GKE

* chhose a distro : redhat atomic, CoreOS tectonic, Mesos
* Build from scratch : good exercice. see "K the hard way"

==== POD A pod of containers
Wrapper around containers
The atom of scheduling containers
Representsan application specific *logical host*
Hosts containers and volumes
each POD has its IP address . Routable (no NAT)
 * multiple containers formats : docker, RKT...
 * simplifiy network
 * group containers
 Containers withina  POD :
 * share port and IPC namespaces
 * talk to each other through localhost


Pods are build from a template and can be replacated
Replicas are identical and repleaceable ...

==== LABELS
Key Value pair, no semantic meanings.
eg : type=FE, version=V2...
You can build queries, selectors with those labels onn the K Dashboard  : selector : version=v2

==== Replicas and ReplicaSets
ReplicaSet #pods=3
* Keep pods running, the right nb of PODs running
* Gives direct control to pod
* grouped by Label Selector

==== Deployment : updates as a service
Status : Beta since K v1.2 :)
You can up//down apps... Deploiy, scale etc..
Edit configuration : command : kubectl edit or kubectl apply
Managed rollouts and rollbacks


==== Services
Pods can run anywhere within a cluster
* pods are ephemeral
* IPs are not stable
How do the pods find each other?

External service gives an external IP addres to the bunch of pofds
Support session affinity
Gets a stable 

Services VIPs are only available inside the cluster
Servicetypes:
 * nodeport : expose a port
 * load balanacer : povisiona cloud load-balancer
DIY laod balancers : ha proxy etc... work too

==== INGRESS for HTTP load balancing (Beta)
Can load balance with the URL : /service1 on pods 1 and /service2 on other ReplicaSets

===== Rollouts
You can setup a new service without interruption




==== Canary


==== VOLUMe
Abstraction of the storage. Can be removed but the storage still exists
The volume exists as long as the POD does. But different lifecycles
* EmptyDir : lives with the POD : will be removed
* HOstPath
* NFS
* Cloud Provider : google cloud etc.
* Persistent VolumeClaim

==== Persistent Columes
Higher level storage Abstraction
Dynamic provisionning (K 1.4)
independant lifetime from the consumers

**** Clustered applications
Pets vs. Cattle

==== Pets : will allow to link a pod to a unique storage
You can bootstrap them
Stable storage
Good for DBs

==== PetSets



=== Config Maps
Manage app configurtion
K is the environment
Manage secrets via K API


===Conclusion
K is open
http://kubernetes.io
@kubernetesio
See github












== Part 2/1 : Kubernetes in Depth

=== MIcroservices how to

==== maven plugins
===== one is good Maven plugin by docker(maven-plugin by com.spotify
mvn package will build the image
docker-maven-plugin : More efficient. You create an envoronment and it will add your code to the other jars you defined before.

**** Docker onbuild : Dockerfile : Can create a parent Dockerfile. Which will for instance create the env, compile etc...

docker tag the app
docker push in the registry
for instance gcr.io (google cloud Registry )
gcloud docker push gcr.io/xxx/tag-name

kubectl run helloworld-service --image=.... -l imagethis,version=1.0 etc.. (-l : --labels that you can query)

To scale:
kubecrl scale deployment --replicas=4
kubecrl expose deployment/helloworld-service --target-port=8080 --port=8080
Creation of a network load balancer , you conect to it. And you can connect now to the

Ingress : http load balancer that loooks inside the HTTP request to do the load balance : by query, url etc...


kubectl get pods # Gives the pods IDs
kubectl logs -f POD_Name # to see the logs


kubectl exec -ti <Nmae of the PODs> /bin/bash # And here you are IN the container :)
Way to do ssh on the container..
just like docker exec -ti ...


DNS i sfirst class citizen
http://hello-service/hello/gabriel will cann 


kubectl get endpoints # gives you the endpoints
And then you can introspect the network, the endpoints and the services 



You can put config in a yaml or a JSON file config. In fact the CLI creates taht file behind the scene
# insert a new App, a new service
kubectl create -f
# upsert an App, a service
kubectl apply -f file
You can bash the deployment : turn-up.sh
with in the .sh
kubectl apply this
kubectl apply that
kubectl apply that
etc...

You can add in the file :
type: LoadBalancer and it will be reachable by other services

Health checks in K : add a livenessProbe in the config file
livenessProbe:
  httpGet:
    path: /
    port: 8080

kubectl edit ...
And it will update the K config :) Rolling update one by one on each POD

To rolllback
kubectl rollout undo deployment helloworld-ui

To see the history
kubectl rollout history deployment helloworld-ui

You have properties in a file
Create config map and put it in K
kubectl create configmap helloworld-v1 --form-file=appplication.properties
You can mount that file in the cluster


VolumeMounts : to mount a volume in an app , in a container


You can crreate a secret
kubectl creat esecret


===== Namespace
Create a Namespace (dev, prod, staging...) for instance

== Federation, quickly
via the Federation Control Pane










== Part 2/2: Multi-region federation










== Developing in ka8s with Continuous Delivery by James Strachan
How java devs can simply use K.
Local development and after

setup your maven project

you need a cluster:
mvn fabric@:cluster-start
** install minikube to run K on a laptop.  And install  kubectl **
You can run a cluster on a laptop

to run your app on K
mvn fabric8:run

Imagine K as an app server. It's not, it's a cluster. But you can imagine that

fabric8 integrated the kubectl with  the usual names :  run install etc... 
Try things as if it was in production

mvn fabric8:run Feels the same as mvn run :)

fabric8: is a way to help java devs to use kubectl. A wrapper
mvn fabric8:deploy undeploy log etc..


=== Service Discovery
You can call a service by its name, in a namespace Use DNS K
http://foo-bar
https://foo-bar
Cool! No need to use dev/prod/staging configs. 

=== logs :
write to Stdout
write a Mbean if yo ucan :)


=== configuration
Put most configuration inside the image
 * Services
 * Secretes

=== testing
You can test your K configs. Because you can do tyos of your ports etc...

You can use arquillian

== Continuous Delivery
James Strachan "The only way of getting good is getting faster on iterate". We need to go fast to have better software


== Microservices : 2 things
For me microservices are two things:
* You can go faster. With microservices each rectanlgle is independant, is a release.
* Have small teams which look to the entire lifecycle of each microservice.

How to automate?
fabric8 helsp you create build, release etc.. Automate and go faster

There is a fabric8 console
fabric8 : you can create a workflow with git jenkins gocs (git repo) nexus etc...
release -> integration testing -> tag docker image -> deploy in staging env (K environment) -> Approve

Lots of things in fabric8 : logging, metrics, ChatOps to communicate, etc...
@jstrachan













= FLEXBOX by Hubert Sablonniere
Get devs and css together
ADD : Autocomplete Drvene developmenet
Css is a real language. Yo uhave to learn it :)


A demo about issues with alignments, float issues
    use in your css :
     display:flex;
    You can use flex-direction:column; /* or row */

    flex:1  : a shortcut for flex-grow:1 ; flex-shrink:1 ; flex-basis:1

How to use flex : first put things in the box. Then distribute the room
    First : add width:xx px; 
    Then add to everybody : flex-grow:1;
    Add width to each cell then distribute the rest of the rom with -grow

    * flex-shrink : evenly remove width to be able to draw everything in the box


    * justify-content : center| flex-end|space-around|space-between ;
    ex : center on the main axis (from flex-direction)

    * align-items:center; Align vertically. Yes we can
    
    * align-self : flex-end|flex-start :
    align one element

        
 * flex-wrap : wrap; : If a box does not fit in the bow it is sent to the next line

        Resources : flexbugs and css tricks (a complete guide)
    
    


== Apache Spark - if only it worked
Marcin Szymaniuk

Hard to make Spark work
Let's share the lesson of why it's sometimes hard

    * Execution model
    Tasks to run
    * Sizing executors
    ex : --executor-cores 3 --executor-memory 10g
    Can be good to rnu multiple tasks in the same jvm
    Many ores leads to problems : hdfs io, gc..
    1-4 cpu is good for a start

    keep in mind memory overhead
    Keep some resources for OS and don't forget yarn overhead

    Be carefull , shuffle : 2g block limit
    Timeout, GC issues...

    atch out for : too much data per task? is Parallelism level large enough?
    groupByKey(nbOfPartition)
    repartittion(nbOfPartition)
    More memory on executor

    
    * Skewed data
Sometimes some tasks are bigger than others
    

    
    * Locality
The issue is to run the task where your data is 

    * Caching

Control nb of partititons
Use mapValues instead of map if you can
Broadcast variables
....    
    
    * Debugging tools
Spark UI : maindebugging tool
HDFS monitoring tool
Aggregagte your logs
Extra java options - observe your GC
Run and test locally
    
    * Local run/tests
    * Challenge and a draw



    
        
