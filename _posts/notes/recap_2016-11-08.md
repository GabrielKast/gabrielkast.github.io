Tuesday 8th of November 2016
TensorFlow university


{{{

}}}
import tensorflow as tf
tf.placeholder
tf.placeholder
...

{{{
# 0.003 learning rate
optimizer = tf.train.GradientDescentOptimizer(0.003)
# loss function
train_step = optimizer.minimize(cross_entropy)
}}}


The NN : layers of sigmoids then softmax at the end


Gradient
Formal derivation to compute the gradience
Gradient is usefull because it point down, at the local minimum
Derivation of each weigth ?

tf has a differed execution model. Nothing is executed

{{{
sess = tf.Session()
sess.run(init)

for i in rnge(1000)
    batch_X, batch_Y = mnist.train.next_batch(1000)
    train_data  ={X : batch_X, Y_ : batch_Y}
    
    # train step
    sess.run(train_step, feed_dict=train_data)

    # DISPLAY part
    # success ?
    
    a, c = sess.run([accuracy, cross_entropy], feed=train_data)


mini batch  : train 1000 at the time.
* Because you run it on GPUs. Bigger matrices are easier on GPUs
* A little bit more stable than 1 at the time



=== Let's add deep learning
Add layers

At the beginning, variable with random. 

interesting, to understand :
tf.variable
.sigmoid
.softmax

RELU : Rectified Linear Unit 
New version : works better
specially on NN
We don't know why it's better :)

Y = tf.nn.relu(tf.matmul(X,W)+b)

Biological neurons don't act as sigmoid but Relus , in fact. :)



98% of accuracy but lots of noise :(
Means you're going too fast, you have to go slower (learning_rate)
The righ approch : start fast and then slow down From : 0.003 to 0.0001


=== Overfitting
With that data, on train , you do no mistake :)

But not good for test data :( .  

==== Let's add regularization
 * dropout method
 shoot Neurones when you train the data. By random. At one layer, train the NN, then shoot x % of the NN
 Use a dropout after each layer. Not on the inputs.
 
 
 pkeep= tf.placeholder(tf.float32) # 0.75 
 Yf = tf.nn.relu(tf.matmul(X,W)+)
# Do a dropout after each layer
Y = tf.nn.dropout(Yf, pkeep)
 
dropout : noise comes back but the test losss is under control. Not going up anymore
RELU : 0.003 to 0.0001 and dropout=0.75 (75%)


* What overfitting means?
  - When you have too many Neurons
  - Not enough data. You always need more
  - Bad network
Maybe 5 layers is too much. But ... why not

But still 98% of accuracy. Never 100% ?


Maybe because we flatten the images
== Convolution layer
To use the shape back
Don't flatten the image
each neuron scan the image 4x4 with a layer of 3 => Weigth(4 x 4 x 3)
Add another NN  W2(4,4,3,2)
W[filter size X, filter size Y, input channels, output channels]
W(4, 4, 3, 2)

Boiling the information down: Convolutional subsampling

Convolution layers : what people use now


==== Convolutional NN
Train is ok but test is going up... => use dropout and more channels.

Hint : first give no freedom to the NN, then free him a little bit. More channels, 

=> 99.3 % 
With dropout you win 2% with test loss :)


Last trick on 


=== batch normalization : the super power :)
Better regularization than dropout

==== Example:
Two sets of data but one is 0-1 and one 0-2 
Let's modify it : principal component Analysis
Please whiten your data. Data whitening.
Rescale and decorrolate the data
Done b hand

Let's do it by NN !  With and additional layer ? ok...
But costly

Do it with batch normalization
Less Costly

Compute average and variance on mini-batch
* Center and rescale logits
* logit = weighted sum + bias
No decorrelation because it's too complex
* But once you have scaled the data, you re-scale it back with learned params
Add learnable scale and offset for each logit, so as to restore expressiveness of data.
Used to have render back the expressiveness.
BN = alpha x + beta
You can take alpha = average and beta = stdev 


Can we still use Gradient after batch normalization? Yes you can

We had a Gaussian input but centered on -1 . Now it's centred on zero :)

So we can use sigmoid , it works great. With relus it still works

Technical things : 
* remove the Bias, because it's not used anymore
* with relu, you do not use alpha, not used
The activation is cleaner, you can go faster
But yo uneed a faster learnign rate
* With Batch normalization, you can remove the dropout, or have less dropout 
* at test time : how do you get avg and stdev?
 (x - avg(x)) / (stdev(x) + epsilon) # epsilon is to avoid divide by 0
 
Tensorflow has a batch_normalization function but it does almost nothing...
tf.moments : computes avg and standard deviation (stdev)

You go from 99.3% to 99.5% :)


Tensorflow : We have seen the low level TF
But tf.learn is more high level and it's easier, less code

To see the tf.learn part too
To learn metrics : yo ucan use MetricPsec in the estimator. The monitor monitors the computation, and then will output stats


== Recurrent Neural Networks
After a layer, use the previous input
State machine
Can fix Weight and Biases
But how to teach them? 
Don't realyy understand but what I might understand is that you take 5 times the same cell.
You can also do several layers (deep learning)



Long term dependency : you can learn things in the context you unroll. All the words in a paragraph, and you guess the last word of the last sentence, when the last word is linked to the first sentence. But not more hat that span
RNN are always deep, lots N.
But it has problem

To solve some problem LSTM: 
Gate : ... forget gate, update gate, result gate
There is many forms of LSTM most of them work great
One of them is Really ok : GRU Gated Reccurent Unit
Cheaper in real life


in TensorFlow
cell = tf.nn.rnn_cell.GRUCell(CELLSIZE)) : defines weights and biases internally
mcell = tf.nn.rnn_cell.MultiCell([cell]*NLAYERS, state_is_tuple=False)
Hr,H = tf.nn.dynamic_rnn(mcell, X, initial_state=Hin)

Reshape ti be able to do the softmax (don't ask me details)
Reuse the same weighs and Biases metrics



Apre midi 

= REACTJS Workshop
Mathieu Ancelin
Sébastien Prunier


Souci install avec npm etc.. 
De plus on commence avec un utilitaire creat-react-app qui marche pas pour moi
Souci de réseau
Et javascript v6 c'est gentil mais il faut prévenir
ça me saoule, je me tire

= Docker Nuts & Bolts

Example : use an image to make your code. Then use the artifact
example : use a maven image
Best : docker volume create maven
Then use the image to have the maven cache

Commands:
{{{
docker images # lists all the images in your system
docker images java # list all java images
docker history <IMAGE> 
}}}

Tip : see how official images are made, read the Dockerfile
eg : openjdk 

Docker compose : running multiple containers
Launch your stack : docker-compose up
Describe : docker-compose.yml

user links or (now) networks in the yaml to compose services, docker images
{{{

}}}

You can use docker to open an IDE, go Docker all the way. Too bad, the demo did not work





Resources : github chanezon docker-tips





















